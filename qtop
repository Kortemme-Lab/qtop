#!/usr/bin/env python2
# -*- mode:python;show-trailing-whitespace:t; -*-

"""
Show which jobs are running on the cluster.

Usage:
    qtop [options]

Options:
    -u --user=NAME
        The name(s) of the user(s) to display information for.  By default, the
        user calling the program will be used.  Specify '*' to see all users.

    -p --project=NAME
        The name(s) of the project(s) to display information for.  By default,
        the projects that user calling the program belongs to will be used.
        Specify '*' to see all projects.

    -i
        Run the program in interactive mode, refreshing qstat data every
        interval seconds

    --interval=INTERVAL
        Number of seconds between interactive refreshes

    --test
        Instead of running qstat, parse from output files in the testing_output
        directory. Usually, this should also be run by specifying a project
        specifically with the --project flag.

"""

import subprocess, getpass, collections, itertools
import os
import sys
import time
import random
from Queue import Queue
from threading import Thread
from blessings import Terminal
import datetime

def ts(td):
    return (td.microseconds + (td.seconds + td.days * 24 * 3600) * 1e6) / 1e6

class TermPrinter(object):
    def __init__(self):
        self.term = Terminal()
        self.string_queue = []

    def queue(self, string):
        self.string_queue.append( str(string) )

    def finalize(self):
        print self.term.clear()

        line_count = 0
        for string in self.string_queue:
            for split_string in string.split('\n'):
                if line_count == 0:
                    print self.term.move(0, 0) + split_string + self.term.clear_eol
                else:
                    print split_string + self.term.clear_eol
                line_count += 1

        self.string_queue = []

term_printer = TermPrinter()

def sh(cmd):
    """
    Run the given command in a shell.

    The command should be a single string containing a shell command.  If the
    command contains the names of any local variables enclosed in braces, the
    actual values of the named variables will be filled in.  (Note that this
    works on variables defined in the calling scope, which is a little bit
    magical.)  Regular braces must be escaped as you would with str.format().
    Also be aware that this approach is vulnerable to shell injection attacks.
    """

    # Figure out what local variables are defined in the calling scope.

    import inspect
    frame = inspect.currentframe()
    try: locals = frame.f_back.f_locals
    finally: del frame

    # Run the given command in a shell.  Return everything written to stdout if
    # the command returns an error code of 0, otherwise raise an exception.

    from subprocess import Popen, PIPE, CalledProcessError
    process = Popen(cmd.format(**locals), shell=True, stdout=PIPE)
    stdout, unused_stderr = process.communicate()
    retcode = process.poll()
    if retcode:
        error = subprocess.CalledProcessError(retcode, cmd)
        error.output = stdout
        raise error
    return stdout.strip()

def choose_user(user=None):
    if user is not None:
        return user
    else:
        import getpass
        return getpass.getuser()

def choose_project(project=None, user=None):
    if project is not None:
        return project
    else:
        if user == None:
            user = choose_user(user)
        return sh('''\
                qconf -suser {user} |
                grep default_project |
                awk '{{print $2}}' ''')

class Task (object):
    def __init__ (self, task_id, fields, store_minimal_information = True):
        self.id = task_id
        self.store_minimal_information = store_minimal_information
        self.parse_task_fields(fields)
        if store_minimal_information:
            self.basic_attrs_to_update = [
                'state',
                'queue', 'slots',
            ]
        else:
            self.basic_attrs_to_update = [
                'state', 'cpu', 'mem', 'io',
                'tckts', 'ovrts', 'otckt',
                'ftckt', 'stckt', 'share',
                'queue', 'slots',
            ]
        self.last_update_time = datetime.datetime.now()

    def parse_task_fields(self, fields):
        self.state = fields[7]

        if self.store_minimal_information:
            if 'r' in self.state:
                self.queue = fields[17]
                self.slots = int(fields[18])
            else:
                self.queue = None
                self.slots = int(fields[14])
            return

        self.last_update_time = datetime.datetime.now()

        if 'r' in self.state:
            if fields[8] == 'NA':
                self.cpu = None
            else:
                cpu_data = [int(x) for x in fields[8].split(':')]
                assert( len(cpu_data) == 4 )
                self.cpu = datetime.timedelta(
                    days = cpu_data[0],
                    hours = cpu_data[1],
                    minutes = cpu_data[2],
                    seconds = cpu_data[3],
                )

            self.mem = fields[9]
            self.io = fields[10]
            self.tckts = fields[11]
            self.ovrts = fields[12]
            self.otckt = fields[13]
            self.ftckt = fields[14]
            self.stckt = fields[15]
            self.share = fields[16]
            self.queue = fields[17]
            self.slots = int(fields[18])

        else:
            self.cpu = None
            self.mem = None
            self.io = None
            self.tckts = fields[8]
            self.ovrts = fields[9]
            self.otckt = fields[10]
            self.ftckt = fields[11]
            self.stckt = fields[12]
            self.share = fields[13]
            self.queue = None
            self.slots = int(fields[14])

    def update(self, other):
        for attr in self.basic_attrs_to_update:
            setattr(self, attr, getattr(other, attr))
        self.last_update_time = datetime.datetime.now()

    def estimated_runtime(self):
        # Returns the CPU timedelta + time since last update
        if self.cpu:
            return datetime.datetime.now() - self.last_update_time + self.cpu
        else:
            return None

class Tasks (object):
    # Each job only holds one Tasks object, but it is split
    # apart for extra readability
    def __init__(self, fields):
        self.all_task_ids = set()
        self.recently_touched_task_ids = set()
        self.active_tasks = {} # Task IDs still appearing in output
        self.inactive_tasks = {} # No longer appearing in output
        self.parse_tasks_fields(fields)

    def count_queued_tasks(self):
        count = 0
        for task in self.active_tasks.values():
            if task.state == 'qw':
                count += 1
        return count

    def count_active_running_tasks(self):
        count = 0
        for task in self.active_tasks.values():
            if task.state == 'r':
                count += 1
        return count

    def parse_tasks_fields(self, fields):
        ja_task_id = fields[-1]

        if len(ja_task_id) > 0:
            new_task_ids = parse_ja_task_id(ja_task_id)
        else:
            new_task_ids = set()

        self.all_task_ids.update( new_task_ids )
        print '%d task ids to update' % (len(new_task_ids))
        for task_id in new_task_ids:
            self.update_task( Task(task_id, fields) )

    def update_task(self, task):
        self.all_task_ids.add(task.id)
        self.recently_touched_task_ids.add(task.id)
        if task.id in self.active_tasks:
            self.active_tasks[task.id].update(task)
        elif task.id in self.inactive_tasks:
            self.active_tasks[task.id] = self.inactive_tasks[task.id]
            del self.inactive_tasks[task.id]
            self.active_tasks[task.id].update(task)
        else:
            self.active_tasks[task.id] = task

    def update(self, other):
        self.all_task_ids.update( other.all_task_ids )
        self.recently_touched_task_ids.update( other.recently_touched_task_ids )

        # Update active tasks dict
        for task_id, task in other.active_tasks.iteritems():
            if task_id in self.active_tasks:
                self.active_tasks[task_id].update(task)
            elif task_id in self.inactive_tasks:
                self.active_tasks[task.id] = self.inactive_tasks[task.id]
                del self.inactive_tasks[task.id]
                self.active_tasks[task.id].update(task)
            else:
                self.active_tasks[task_id] = task

        # Update inactive tasks dict
        for task_id, task in other.inactive_tasks.iteritems():
            if task_id in self.inactive_tasks:
                self.inactive_tasks[task_id].update(task)
            elif task_id in self.active_tasks:
                self.inactive_tasks[task.id] = self.active_tasks[task.id]
                del self.active_tasks[task.id]
                self.inactive_tasks[task.id].update(task)
            else:
                self.inactive_tasks[task_id] = task

    def mark_inactive_tasks(self):
        # Create a separate list of IDs ahead of time,
        # because we may delete some and modify the dict
        # as we go, which would break a direct iterator
        task_ids = self.active_tasks.keys()

        for task_id in task_ids:
            if task_id not in self.recently_touched_task_ids:
                task = self.active_tasks[task_id]
                del self.active_tasks[task_id]
                self.inactive_tasks[task_id] = task
        self.recently_touched_task_ids = set()

    def mean_time_per_inactive_task(self):
        # Returns the mean estimated run time of each inactive task with a time > 0
        total_time = datetime.timedelta()
        count = 0.0
        for task in self.inactive_tasks.values():
            runtime = task.estimated_runtime()
            if runtime:
                total_time += runtime
                count += 1.0
        if count == 0.0:
            return None
        else:
            return datetime.timedelta(seconds = ts(total_time) / count)

    def mean_time_per_active_running_task(self):
        # Returns the mean estimated run time of each inactive task with a time > 0
        total_time = datetime.timedelta()
        count = 0.0
        for task in self.active_tasks.values():
            if task.state == 'r':
                runtime = task.estimated_runtime()
                if runtime:
                    total_time += runtime
                    count += 1.0
        if count == 0.0:
            return None
        else:
            return datetime.timedelta(seconds = ts(total_time) / count)

def parse_ja_task_id(ja_task_id):
    task_ids = set()
    for s in ja_task_id.split(','):
        if '-' in s:
            assert( ':' in s )
            range_str, step = s.strip().split(':')
            range_start, range_end = range_str.split('-')
            for x in xrange( int(range_start), int(range_end)+1, int(step) ):
                task_ids.add(x)
        else:
            task_ids.add( int(s) )

    return task_ids

class Job (object):
    def __init__(self, qstat_line):
        fields = qstat_line.split()
        try:
            self.parse_job_fields(fields)
        except:
            print 'Error parsing qstat line:'
            print '  ', qstat_line
            raise

        self.tasks = Tasks(fields)

        # When a job object is updated, these will all be copied
        # from the new job to this object
        self.basic_attrs_to_update = [
            'id', 'prior', 'ntckts', 'user', 'project',
            'department',
            'short_name'
        ]

        self.state = None
        self.full_name = None

    def parse_job_fields(self, fields):
        self.id = long(fields[0])
        self.prior = fields[1]
        self.ntckts = fields[2]
        self.name = fields[3]
        self.short_name = self.name
        self.user = fields[4]
        self.project = fields[5]
        self.department = fields[6]

    def update(self, other):
        for attr in self.basic_attrs_to_update:
            setattr(self, attr, getattr(other, attr))
        self.tasks.update(other.tasks)

    def set_qstat_data(self, qstat_data):
        self.qstat_data = qstat_data

        if 'job_name' in qstat_data:
            self.full_name = qstat_data['job_name']
            self.name = self.full_name

    def after_update(self):
        self.tasks.mark_inactive_tasks()

def enhance_job_data(job_id_queue, results_queue, testing_mode):
    while True:
        if not job_id_queue.empty():
            job_id = job_id_queue.get()

            if testing_mode:
                qstat_file = 'testing_output/job_specific_output/%d' % job_id
                if os.path.isfile(qstat_file):
                    with open(qstat_file, 'r') as f:
                        qstat_output = f.readlines()
                else:
                    qstat_output = None
            else:
                try:
                    qstat_output = sh('qstat -j %d' % job_id).split('\n')
                except:
                    qstat_output = None

            if qstat_output:
                qstat_data = {}
                for line in qstat_output[1:]:
                    fields = line.split(':')
                    if len(fields) == 2:
                        qstat_data[fields[0].strip()] = fields[1].strip()
                if len(qstat_data) > 0:
                    results_queue.put(qstat_data)
                    time.sleep(15.0) # Only run qstat -j for one job every this n seconds
        else:
            time.sleep(0.1) # Check queue more often, as this is easy

class Jobs (object):
    def __init__(self, testing_mode, project, user, background_update = False):
        self.jobs = {}
        self.project = project
        self.test_iteration = 0
        self.testing_mode = testing_mode
        self.user = user

        self.jobs_to_update = Queue()
        self.updated_jobs_results = Queue()
        if background_update:
            self.job_data_updater = Thread(
                target = enhance_job_data,
                args = (
                    self.jobs_to_update,
                    self.updated_jobs_results,
                    self.testing_mode)
            )
            self.job_data_updater.daemon = True
            self.job_data_updater.start()

        self.old_job_ids = set()

    def get_user_jobs(self):
        return_jobs = []
        for job in self.jobs.values():
            if job.user == self.user:
                return_jobs.append(job)
        return return_jobs

    def estimated_time_left(self, job_tup):
        # Returns estimated time left to complete all jobs
        # associated with job_tup
        mean_inactive_time = self.mean_time_per_inactive_task(job_tup)
        if not mean_inactive_time:
            return None
        mean_active_time = self.mean_time_per_active_running_task(job_tup)
        if not mean_active_time:
            return None

        active_running_tasks_count = self.count_active_running_tasks(job_tup)
        queued_tasks_left = self.count_queued_tasks(job_tup)

        return datetime.timedelta( seconds =
             max(0, ts(mean_inactive_time) - ts(mean_active_time))
            +
            ts(mean_inactive_time) * queued_tasks_left / float(active_running_tasks_count)
        )

    def count_queued_tasks(self, job_tup):
        jobs = self.map_job_tups_to_jobs()[job_tup]
        count = 0
        for job in jobs:
            count += job.tasks.count_queued_tasks()
        return count

    def count_active_running_tasks(self, job_tup):
        jobs = self.map_job_tups_to_jobs()[job_tup]
        count = 0
        for job in jobs:
            count += job.tasks.count_active_running_tasks()
        return count

    def mean_time_per_inactive_task(self, job_tup):
        # Returns mean time per inactive task
        jobs = self.map_job_tups_to_jobs()[job_tup]
        mean_runtimes = []
        for job in jobs:
            mean_time = job.tasks.mean_time_per_inactive_task()
            if mean_time:
                mean_runtimes.append(ts(mean_time))
        if len(mean_runtimes) == 0:
            return None
        else:
            return datetime.timedelta( seconds = float(sum(mean_runtimes)) / float(len(mean_runtimes)) )

    def mean_time_per_active_running_task(self, job_tup):
        # Returns mean time per inactive task
        jobs = self.map_job_tups_to_jobs()[job_tup]
        mean_runtimes = []
        for job in jobs:
            mean_time = job.tasks.mean_time_per_active_running_task()
            if mean_time:
                mean_runtimes.append(ts(mean_time))
        if len(mean_runtimes) == 0:
            return None
        else:
            return datetime.timedelta( seconds = float(sum(mean_runtimes)) / float(len(mean_runtimes)) )

    def map_job_tups_to_jobs(self, search_project=None, search_user=None):
        m = {}
        for job in self.jobs.values():
            if search_project != None and job.project != search_project:
                continue
            if search_user != None and job.user != search_user:
                continue

            short_name = job.short_name
            user = job.user
            job_tup = (short_name, user)
            if job_tup not in m:
                m[job_tup] = []
            m[job_tup].append(job)
        return m

    def job_tups(self, project=None, user=None):
        return self.map_job_tups_to_jobs(search_project=project, search_user=user).keys()

    def count_dict_by_name(self, project):
        count_dict = {}
        users = set()
        for job in self.jobs.values():
            if project and job.project != project:
                continue

            task_counts = {
                'lab_q_tasks' : 0,
                'long_q_tasks' : 0,
                'short_q_tasks' : 0,
                'waiting_q_tasks' : 0,
                'other_q_tasks' : 0,
                'slots' : 0,
            }

            # Iterate through all tasks in job and count active ones
            for task in job.tasks.active_tasks.values():
                if task.state == 'r' or task.state == 'hr':
                    if task.queue.startswith('lab.q'):
                        task_counts['lab_q_tasks'] += 1
                        task_counts['slots'] += task.slots
                    elif task.queue.startswith('long.q'):
                        task_counts['long_q_tasks'] += 1
                        task_counts['slots'] += task.slots
                    elif task.queue.startswith('short.q'):
                        task_counts['short_q_tasks'] += 1
                        task_counts['slots'] += task.slots
                    else:
                        raise Exception(
                            "Couldn't recognize queue type %s (job %d task %d)" % (task.queue, job.id, task.id)
                        )
                elif 'qw' in task.state: # Covers 'qw' and 'hqw'
                    task_counts['waiting_q_tasks'] += 1
                else:
                    task_counts['other_q_tasks'] += 1

            # If we counted any active tasks, then count them for this job
            # Job counts are based on a unique job short name and username (tuple)
            all_tasks = task_counts['waiting_q_tasks'] + task_counts['lab_q_tasks'] + task_counts['short_q_tasks'] + task_counts['long_q_tasks'] + task_counts['other_q_tasks']
            running_tasks = task_counts['lab_q_tasks'] + task_counts['short_q_tasks'] + task_counts['long_q_tasks']
            if all_tasks > 0:
                short_name = job.short_name
                user = job.user
                users.add(user)
                job_tup = (short_name, user)

                if job_tup in count_dict:
                    count_dict[job_tup]['jobs'] += 1
                    count_dict[job_tup]['all_tasks'] += all_tasks
                    count_dict[job_tup]['running_tasks'] += running_tasks
                    count_dict[job_tup]['lab_q_tasks'] += task_counts['lab_q_tasks']
                    count_dict[job_tup]['short_q_tasks'] += task_counts['short_q_tasks']
                    count_dict[job_tup]['long_q_tasks'] += task_counts['long_q_tasks']
                    count_dict[job_tup]['full_names'].add(job.full_name)
                    count_dict[job_tup]['waiting_q_tasks'] += task_counts['waiting_q_tasks']
                    count_dict[job_tup]['other_q_tasks'] += task_counts['other_q_tasks']
                    count_dict[job_tup]['slots'] += task_counts['slots']
                else:
                    count_dict[job_tup] = {
                        'jobs' : 1,
                        'all_tasks' : all_tasks,
                        'running_tasks' : running_tasks,
                        'lab_q_tasks' : task_counts['lab_q_tasks'],
                        'short_q_tasks' : task_counts['short_q_tasks'],
                        'long_q_tasks' : task_counts['long_q_tasks'],
                        'waiting_q_tasks' : task_counts['waiting_q_tasks'],
                        'other_q_tasks' : task_counts['other_q_tasks'],
                        'slots' : task_counts['slots'],
                        'full_names' : set([job.full_name]),
                    }

        self.counted_users = users
        # Process full names to be common string by position
        # It could be cool to make this instead add wildcards more fancily
        for job_tup, inner_dict in count_dict.iteritems():
            if None in inner_dict['full_names']:
                inner_dict['full_names'].remove(None)

            if len(inner_dict['full_names']) == 0:
                inner_dict['common_full_name'] = job_tup[0]
                continue

            full_names = [
                y for x, y in sorted(
                    [(len(name), name) for name in inner_dict['full_names']
                 ], reverse=True )
            ]

            common_full_name = ''
            for i, char in enumerate(full_names[0]):
                add_char = True
                for full_name in full_names[1:]:
                    if len(full_name) <= i or char != full_name[i]:
                        add_char = False
                if add_char:
                    common_full_name += char
                elif common_full_name[-1] != '*':
                    common_full_name += '*'

            inner_dict['common_full_name'] = common_full_name

        return count_dict

    def count_jobs_by_name(self, project):
        count_dict = self.count_dict_by_name(project)
        return_list = []
        for job_tup in count_dict:
            job_short_name, job_user = job_tup
            return_list.append( (
                count_dict[job_tup]['slots'],
                count_dict[job_tup]['jobs'],
                count_dict[job_tup]['lab_q_tasks'],
                count_dict[job_tup]['long_q_tasks'],
                count_dict[job_tup]['short_q_tasks'],
                count_dict[job_tup]['waiting_q_tasks'],
                count_dict[job_tup]['other_q_tasks'],
                job_user,
                count_dict[job_tup]['common_full_name'],
            ) )

        return_list.sort(reverse=True)
        return return_list

    def num_users(self, project=None):
        if not project:
            project = self.project

        users = set()
        for job in self.jobs.values():
            if project and job.project != project:
                continue
            users.add(job.user)
        return len(users)

    def percent_used_by_project(self, project):
        project_tasks = self.count_jobs_by_name(project)
        all_tasks = self.count_jobs_by_name(None)

        all_running_tasks = sum([x[0] for x in all_tasks])
        project_running_tasks = sum([x[0] for x in project_tasks])

        return 100.0 * float(project_running_tasks) / float(all_running_tasks)

    def update(self):
        if self.testing_mode:
            input_data = read_from_test_output(self.test_iteration)
            self.test_iteration += 1
        else:
            input_data = sh('qstat -ext -u \\*').split('\n')

        new_job_ids = set()
        parsed_job_ids = set()
        total_line_count = len(input_data[2:])
        line_count = 1
        for line in input_data[2:]:
            print line
            job = Job(line)
            print 'Finished %d lines (%d total)\n' % (line_count, total_line_count)
            line_count += 1
            parsed_job_ids.add(job.id)
            if job.id in self.jobs:
                self.jobs[job.id].update(job)
            else:
                self.jobs[job.id] = job
                new_job_ids.add(job.id)

        missing_job_ids = self.old_job_ids.difference(parsed_job_ids)
        for job_id in missing_job_ids:
            assert( job_id in self.jobs )
            self.jobs[job_id].state = 'finished'
        self.old_job_ids = parsed_job_ids

        new_job_ids_list = list(new_job_ids)
        random.shuffle(new_job_ids_list)
        for job_id in new_job_ids_list:
            job = self.jobs[job_id]
            if job.project == self.project and len(job.name) >= 10:
                self.jobs_to_update.put(job_id)

        # Get new qstat_data results from queue
        while not self.updated_jobs_results.empty():
            qstat_data = self.updated_jobs_results.get()
            job_id = long( qstat_data['job_number'] )
            self.jobs[job_id].set_qstat_data( qstat_data )

        # Mark inactive tasks
        for job in self.jobs.values():
            job.after_update()

    def __iter__(self):
        self.sorted_job_ids = [
            x for x,y in self.jobs.iteritems() if y.state != 'finished'
        ]
        self.sorted_job_ids.sort(reverse=True)
        return self

    # Python 3 compatibility
    def __next__(self):
        return self.next()

    def next(self):
        if len(self.sorted_job_ids) == 0:
            raise StopIteration
        return self.jobs[ self.sorted_job_ids.pop() ]

def read_from_test_output(test_iteration):
    testing_output_dir = 'testing_output'
    assert( os.path.isdir( testing_output_dir ) )
    test_files = [
        os.path.join(testing_output_dir, f)
        for f in sorted(os.listdir(testing_output_dir))
        if 'job_specific_output' not in f
    ]

    term_printer.queue( test_files[test_iteration % len(test_files)] )
    with open(test_files[test_iteration % len(test_files)], 'r') as f:
        return f.readlines()

def display(data):
    max_lengths = []
    strings_to_print = []
    for i, l in enumerate(data):
        strings_to_print.append([])
        for j, s in enumerate(l):
            s = str(s)
            strings_to_print[i].append(s)
            s_len = len(s)
            while len(max_lengths) < j+1:
                max_lengths.append(0)
            if s_len > max_lengths[j]:
                max_lengths[j] = s_len

    for row in strings_to_print:
        row_str = ''
        for i, x in enumerate(row):
            if i >= len(row) - 2:
                row_str += '%s ' % x.ljust(max_lengths[i])
            else:
                row_str += '%s ' % x.rjust(max_lengths[i])

        term_printer.queue(
            row_str[:-1] # Remove trailing space
        )

def display_jobs(jobs):
    job_display_list = jobs.count_jobs_by_name(jobs.project)

    if len(job_display_list) == 0:
        return

    if jobs.project:
        term_printer.queue( "Currently running jobs by {0} {1} users:".format( len(jobs.counted_users), jobs.project ) )
    else:
        term_printer.queue( "Currently running jobs by {0} users:".format( len(jobs.counted_users) ) )

    display_list = [ [
        'Slots', 'Jobs',
        'lab.q', 'long.q', 'short.q',
        'Queued',
        'Other',
        'User',
        'Name',
    ] ]

    display_list.extend( job_display_list )
    display( display_list )

    # Job tuples are (job.short_name, job.user)
    # Allows grouping of jobs with same name
    job_tups = jobs.job_tups(project=jobs.project, user=jobs.user)
    if len(job_tups) > 0:
        mean_runtimes_display_list = [ [
            'Name',
            'Mean runtime',
            'Estimated time till completion',
        ]]

        for job_tup in job_tups:
            mean_time = jobs.mean_time_per_inactive_task(job_tup)
            est_time_left = jobs.estimated_time_left(job_tup)
            if mean_time or est_time_left:
                mean_runtimes_display_list.append( [
                    job_tup[0], mean_time,
                    est_time_left,
                ] )

        if len(mean_runtimes_display_list) >= 2:
            if len(job_tups) == 1:
                term_printer.queue( "\nUser {0} is currently running 1 unique job:".format( jobs.user ) )
            elif len(job_tups) > 1:
                term_printer.queue( "\nUser {0} is currently running {1} unique jobs:".format( jobs.user, len(job_tups) ) )
            display( mean_runtimes_display_list )

def run( project, user, testing_mode=False, interval=0 ):
    # Figure out what projects to display information for.  If the user
    # specifies one or more projects on the command line, use them.  Otherwise
    # use the projects the current user belongs to.

    # Only start background updating thread if interval > 0
    if float(interval) > 0.0:
        background_update = True
    else:
        background_update = False

    jobs = Jobs(testing_mode, project, user, background_update = background_update )
    while True:
        jobs.update()

        term_printer.queue( time.strftime("%a, %d %b %Y %H:%M:%S ") )
        if jobs.project:
            term_printer.queue(
                '\nPercentage of overall slots in use by project %s:' % (jobs.project)
            )
            term_printer.queue( '%.2f%%\n' % jobs.percent_used_by_project(jobs.project) )
        display_jobs(jobs)
        term_printer.finalize()

        if float(interval) == 0.0:
            return
        else:
            # Let's rest this thread
            time.sleep(float(interval))

if __name__ == '__main__':
    import docopt
    args = docopt.docopt(__doc__)

    testing_mode = args['--test']
    interactive_mode = args['-i']
    args_interval = args['--interval']
    user = choose_user(args['--user'])
    project = choose_project(args['--project'], user=user)

    if not interactive_mode and not args_interval:
        interval = 0.0
    elif interactive_mode and not args_interval:
        interval = 60.0
    elif args_interval:
        interval = float( args_interval )

    if interval < 60.0 and interval != 0.0 and not testing_mode:
        raise Exception(
            'ERROR: Update interval cannot be less than 60 seconds'
        )

    run( project, user, testing_mode=testing_mode, interval=interval )
